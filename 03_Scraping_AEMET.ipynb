{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping. Obtención de predicción de radiación desde AEMET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para el modelo de producción, se obtendrán los datos proporcionados por la AEMET referentes a la radiación global diaria. Estos datos, son suministrados directamente desde la API de la plataforma OpenData de la agencia, facilitando su manipulación.\n",
    "\n",
    "Lo que buscamos con ello, es recojer e introducir los parámetros de entrada diarios de radiación que luego serán introducidos en el modelo entrenado, permitiendonos predecir la producción eléctrica que tendremos a lo largo del día.\n",
    "\n",
    "Éste script solo se ejecutará una vez al día, ya que luego los datos serán almacenados en un archivo CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tras registrarnos en la plataforma, se nos suministra una \"api_key\" que nos permite solicitar los datos deseados a la misma, descargandose estos en formato de texto plano."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = \"eyJhbGciOiJIUzI1NiJ9.eyJzdWIiOiJhYmhfbWFzcGFAaG90bWFpbC5jb20iLCJqdGkiOiIxNjU4M2UxYS02YjhmLTQwMjctYTU4YS02YmQ2ZjVhM2U3MGMiLCJpc3MiOiJBRU1FVCIsImlhdCI6MTU3NzE4NDI0MiwidXNlcklkIjoiMTY1ODNlMWEtNmI4Zi00MDI3LWE1OGEtNmJkNmY1YTNlNzBjIiwicm9sZSI6IiJ9.Ov09paQ1cXsLACg_FKe3qdFpUyI-yAuXhu74rUmpD-w\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nos conectamos mediante la lbreria \"request\" a la plataforma, y descargamos los datos del servidor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Url de los datos que deseamos obtener, que son los pertenecientes a la red especial de radiación.\n",
    "\n",
    "url = \"https://opendata.aemet.es/opendata/api/red/especial/radiacion/\"\n",
    "\n",
    "querystring = {\"api_key\":api_key}\n",
    "\n",
    "headers = {'cache-control': \"no-cache\"}\n",
    "\n",
    "# Tras la solicitud, obtenemos un json con la url donde están publicados los datos a descargar. La url\n",
    "# se encuentra en el campo dato.\n",
    "\n",
    "response = requests.request(\"GET\", url, headers=headers, params=querystring)\n",
    "\n",
    "# Descargamos los datos desde la url suministrada.\n",
    "\n",
    "url_data = requests.request(\"GET\", eval(response.text)[\"datos\"], headers=headers, params=querystring).text\n",
    "\n",
    "# Todos los datos, se encuentran entrecomillados, por ello pasamos a eliminarlas.\n",
    "url_data = url_data.replace('\"', '') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tratamos los datos suministrados, transformandolos de texto plano a un dataframe, que luego será guardado en un archivo CSV, el cuál nos valdrá de entrada tanto para el modelo como para la visualización de los mismos en el Dashboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "# Separamos la información por líneas.\n",
    "\n",
    "url_data = url_data.splitlines()\n",
    "\n",
    "# Guardamos la fecha del día predicción, que se encuentra en el segundo elemento de la lista.\n",
    "\n",
    "date = url_data[1]\n",
    "date_splt = url_data[1].split('-')\n",
    "\n",
    "# Obtenemos las horas de predicción que se nos ha suministrado. Normalmente oscilan entre las 5-20 horas.\n",
    "\n",
    "str_hour = url_data[2].split(';').index('Tipo')\n",
    "end_hour = url_data[2].split(';').index('SUMA')\n",
    "\n",
    "hours = url_data[2].split(';')[str_hour+1:end_hour]\n",
    "\n",
    "# Creamos la cabecera de nuestro dataframe.\n",
    "\n",
    "station_df = pd.DataFrame(columns =['City', 'Station', 'Date', 'Global_Radiation', \n",
    "                                   'Diffuse_Radiation', 'Ultraviolet'])\n",
    "\n",
    "# Recorremos las líneas de los datos, desde las 3 hasta el final. Las dos primeras son la cabecera que\n",
    "# solo aportan la fecha y el nombre del archivo.\n",
    "\n",
    "for station_data in url_data[3:]:\n",
    "    \n",
    "    station_data = station_data.split(';') # Separamos la información por ;\n",
    "    \n",
    "    if station_data[0] == 'Madrid, Ciudad Universitaria': station_data[0] = 'Madrid'\n",
    "   \n",
    "    city = station_data[0]    # Guardamos el nombre de la ciudad\n",
    "    station = station_data[1] # Guardamos la fecha de los datos\n",
    "    \n",
    "    indx_GL = station_data.index('GL') + 1  # Guardamos los indices donde comienzan los datos de GL\n",
    "    indx_DF = station_data.index('DF') + 1  # Guardamos los indices donde comienzan los datos de DF\n",
    "    indx_UV = station_data.index('UVB') + 2 # Guardamos los indices donde comienzan los datos de UVB\n",
    "    \n",
    "    # Recorremos la lista de los elementos, y guardamos los datos en un dataframe. El formato siempre será el\n",
    "    # el mismo: Ciudad, estación, hora, dato de GL, dato de DF, dato UV. De esta manera guardamos todos los datos\n",
    "    # en un mismo Dataframe.\n",
    "    \n",
    "    for i in range(len(hours)):\n",
    "        \n",
    "        date_hour = datetime.datetime(int('20'+date_splt[2]),int(date_splt[1]),int(date_splt[0]),\n",
    "                                      int(hours[i]),0,0)\n",
    "        \n",
    "        station_df.loc[i+len(station_df)] = [city, station, date_hour, station_data[i+indx_GL], \n",
    "                                             station_data[i+indx_DF], station_data[i+indx_UV]]\n",
    "\n",
    "# Cambiamos el formato de la fecha para que comience por el día y no por el año.\n",
    "station_df['Date'] = station_df['Date'].apply(lambda x: x.strftime(\"%d-%m-%Y %H:%M:%S\"))\n",
    "\n",
    "# Ordenamos los valores y reseteamos los índices.\n",
    "station_df.sort_values(by=['City', 'Date'], inplace=True)\n",
    "station_df.reset_index(inplace=True, drop=True)\n",
    "\n",
    "# Unimos las columnas de fecha y hora y las convertimos en un datetime. Luego eliminamos la columna de 'Hour'\n",
    "\n",
    "#station_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Procedemos a guardar el Data Frame en un archivo CSV, que podremos utilizar posteriormente con mayor facilidad para la visualización y predicción de la producción."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardamos el archivo en local\n",
    "station_df.to_csv('/home/dsc/Documents/TFM-Sun-Power-Prediction/dataset_aemet/'+ date + '.csv', \n",
    "                  sep=';', index=False)\n",
    "\n",
    "# Guardamos el nombre del archivo para subirlo a Drive\n",
    "file_name = '/home/dsc/Documents/TFM-Sun-Power-Prediction/dataset_aemet/'+ date + '.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cargarmos los modelos entrenados y les pasamos los datos de radiación descargados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cargamos los modelos ya entrenados para volverlos a utilizar. Tras descargar y tratar los datos de la AEMET, procederemos a pasarlos por el modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "regDG_VDC = pickle.load(open(\"regDG_VDC.pickle\", \"rb\"))\n",
    "regDG_CDC = pickle.load(open(\"regDG_CDC.pickle\", \"rb\"))\n",
    "regDG_PDC = pickle.load(open(\"regDG_PDC.pickle\", \"rb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ejecutamos la predicción de los datos mediante el modelo. Pero antes, recabamos los datos del CSV ya guardado y, tras cambiar el tipo de datos de String a float32, realizamos una interpolación para eliminar los NaN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "aemet_data = pd.read_csv(file_name,sep=';') # Recuperamos el archivo CSV anteriormente guardado.\n",
    "\n",
    "aemet_data[['Global_Radiation', 'Diffuse_Radiation',\n",
    "       'Ultraviolet']].astype(\"float32\") # Transformamos los datos a tipo float32\n",
    "\n",
    "aemet_data.interpolate(inplace=True) # Interpolamos para eliminar los datos NaN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Realizamos la predicción de los valores de radiación, en función de los valores de entrada. Es muy probable que muchos de los valores estén a 0 por motivos desconocidos, por lo que la predicción no tendrá el acierto esperado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "aemet_data['Predict_VDC']= pd.DataFrame(regDG_VDC.predict(aemet_data[['Global_Radiation', \n",
    "                                                                      'Diffuse_Radiation','Ultraviolet']]))\n",
    "\n",
    "aemet_data['Predict_CDC']= pd.DataFrame(regDG_CDC.predict(aemet_data[['Global_Radiation', \n",
    "                                                                      'Diffuse_Radiation','Ultraviolet']]))\n",
    "\n",
    "aemet_data['Predict_PDC']= pd.DataFrame(regDG_PDC.predict(aemet_data[['Global_Radiation', \n",
    "                                                                      'Diffuse_Radiation', 'Ultraviolet']]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Guardamos nuevamente los datos en formato CSV en local."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "aemet_data.to_csv (file_name, sep=';')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subir archivos a google drive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Subiremos los archivos a Google Drive para poder trabajar con ellos desde el Dashboard implementado en Tableau, el cual corre sobre un PC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importamos las librerias\n",
    "\n",
    "from pydrive.auth import GoogleAuth\n",
    "from pydrive.drive import GoogleDrive\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your browser has been opened to visit:\n",
      "\n",
      "    https://accounts.google.com/o/oauth2/auth?client_id=93417544293-1s6ei4pnt9f84qua81cv8g062vsgpuof.apps.googleusercontent.com&redirect_uri=http%3A%2F%2Flocalhost%3A8080%2F&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&access_type=offline&response_type=code\n",
      "\n",
      "Authentication successful.\n"
     ]
    }
   ],
   "source": [
    "# Nos autentificamos en Google Drive y creamos la conección.\n",
    "g_login = GoogleAuth()\n",
    "g_login.LocalWebserverAuth()\n",
    "drive = GoogleDrive(g_login)\n",
    "g_login.LocalWebserverAuth()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Abrimos el archivo que queremos subir, y lo subimos a la ubicacion de la carpeta compartida mediante ID\n",
    "with open(file_name,\"r\") as file:\n",
    "    \n",
    "    file_drive = drive.CreateFile({'title':os.path.basename(file.name), \n",
    "                                   \"parents\": [{\"kind\": \"drive#fileLink\", \n",
    "                                                \"id\": \"1-hoNgYsZW1h4gYKY9uAPrJfEAaTOhGCW\"}]})  \n",
    "    file_drive.SetContentString(file.read()) \n",
    "    file_drive.Upload() "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
