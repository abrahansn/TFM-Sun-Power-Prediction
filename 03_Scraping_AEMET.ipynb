{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping. Obtención de predicción de radiación desde AEMET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para el modelo de producción, se obtendrán los datos proporcionados por la AEMET referentes a la radiación global diaria. Estos datos, son suministrados directamente desde la API de la plataforma OpenData de la agencia, facilitando su manipulación.\n",
    "\n",
    "Lo que buscamos con ello, es recojer e introducir los parámetros de entrada diarios de radiación que luego serán introducidos en el modelo entrenado, permitiendonos predecir la producción eléctrica que tendremos a lo largo del día.\n",
    "\n",
    "Éste script solo se ejecutará una vez al día, ya que luego los datos serán almacenados en un archivo CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tras registrarnos en la plataforma, se nos suministra una \"api_key\" que nos permite solicitar los datos deseados a la misma, descargandose estos en formato de texto plano."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = \"eyJhbGciOiJIUzI1NiJ9.eyJzdWIiOiJhYmhfbWFzcGFAaG90bWFpbC5jb20iLCJqdGkiOiIxNjU4M2UxYS02YjhmLTQwMjctYTU4YS02YmQ2ZjVhM2U3MGMiLCJpc3MiOiJBRU1FVCIsImlhdCI6MTU3NzE4NDI0MiwidXNlcklkIjoiMTY1ODNlMWEtNmI4Zi00MDI3LWE1OGEtNmJkNmY1YTNlNzBjIiwicm9sZSI6IiJ9.Ov09paQ1cXsLACg_FKe3qdFpUyI-yAuXhu74rUmpD-w\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nos conectamos mediante la lbreria \"request\" a la plataforma, y descargamos los datos del servidor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Url de los datos que deseamos obtener, que son los pertenecientes a la red especial de radiación.\n",
    "\n",
    "url = \"https://opendata.aemet.es/opendata/api/red/especial/radiacion/\"\n",
    "\n",
    "querystring = {\"api_key\":api_key}\n",
    "\n",
    "headers = {'cache-control': \"no-cache\"}\n",
    "\n",
    "# Tras la solicitud, obtenemos un json con la url donde están publicados los datos a descargar. La url\n",
    "# se encuentra en el campo dato.\n",
    "\n",
    "response = requests.request(\"GET\", url, headers=headers, params=querystring)\n",
    "\n",
    "# Descargamos los datos desde la url suministrada.\n",
    "\n",
    "url_data = requests.request(\"GET\", eval(response.text)[\"datos\"], headers=headers, params=querystring).text\n",
    "\n",
    "# Todos los datos, se encuentran entrecomillados, por ello pasamos a eliminarlas.\n",
    "url_data = url_data.replace('\"', '') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tratamos los datos suministrados, transformandolos de texto plano a un dataframe, que luego será guardado en un archivo CSV, el cuál nos valdrá de entrada tanto para el modelo como para la visualización de los mismos en el Dashboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'splitlines'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-e425a1a7882d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Separamos la información por líneas.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0murl_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0murl_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplitlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Guardamos la fecha del día predicción, que se encuentra en el segundo elemento de la lista.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'splitlines'"
     ]
    }
   ],
   "source": [
    "# Separamos la información por líneas.\n",
    "\n",
    "url_data = url_data.splitlines()\n",
    "\n",
    "# Guardamos la fecha del día predicción, que se encuentra en el segundo elemento de la lista.\n",
    "\n",
    "date = url_data[1]\n",
    "\n",
    "# Obtenemos las horas de predicción que se nos ha suministrado. Normalmente oscilan entre las 5-20 horas.\n",
    "\n",
    "str_hour = url_data[2].split(';').index('Tipo')\n",
    "end_hour = url_data[2].split(';').index('SUMA')\n",
    "\n",
    "hours = url_data[2].split(';')[str_hour+1:end_hour]\n",
    "\n",
    "# Creamos la cabecera de nuestro dataframe.\n",
    "\n",
    "station_df = pd.DataFrame(columns =['City', 'Station', 'Hour', 'Global_Radiation', \n",
    "                                   'Diffuse_Radiation', 'Ultraviolet'])\n",
    "\n",
    "# Recorremos las líneas de los datos, desde las 3 hasta el final. Las dos primeras son la cabecera que\n",
    "# solo aportan la fecha y el nombre del archivo.\n",
    "\n",
    "for station_data in url_data[3:]:\n",
    "    \n",
    "    station_data = station_data.split(';') # Separamos la información por ;\n",
    "    \n",
    "    city = station_data[0]    # Guardamos el nombre de la ciudad\n",
    "    station = station_data[1] # Guardamos la fecha de los datos\n",
    "    \n",
    "    indx_GL = station_data.index('GL') + 1  # Guardamos los indices donde comienzan los datos de GL\n",
    "    indx_DF = station_data.index('DF') + 1  # Guardamos los indices donde comienzan los datos de DF\n",
    "    indx_UV = station_data.index('UVB') + 2 # Guardamos los indices donde comienzan los datos de UVB\n",
    "    \n",
    "    # Recorremos la lista de los elementos, y guardamos los datos en un dataframe. El formato siempre será el\n",
    "    # el mismo: Ciudad, estación, hora, dato de GL, dato de DF, dato UV. De esta manera guardamos todos los datos\n",
    "    # en un mismo Dataframe.\n",
    "    \n",
    "    for i in range(len(hours)):\n",
    "        \n",
    "        station_df.loc[i+len(station_df)] = [city, station, hours[i], station_data[i+indx_GL], \n",
    "                                             station_data[i+indx_DF], station_data[i+indx_UV]]\n",
    "\n",
    "station_df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Procedemos a guardar el Data Frame en un archivo CSV, que podremos utilizar posteriormente con mayor facilidad para la visualización y predicción de la producción."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardamos el archivo en local\n",
    "station_df.to_csv('/home/dsc/Documents/TFM-Sun-Power-Prediction/dataset_aemet/'+ date + '.csv', \n",
    "                  sep=';', index=False)\n",
    "\n",
    "# Guardamos el nombre del archivo para subirlo a Drive\n",
    "file_name = '/home/dsc/Documents/TFM-Sun-Power-Prediction/dataset_aemet/'+ date + '.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subir archivos a google drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your browser has been opened to visit:\n",
      "\n",
      "    https://accounts.google.com/o/oauth2/auth?client_id=93417544293-1s6ei4pnt9f84qua81cv8g062vsgpuof.apps.googleusercontent.com&redirect_uri=http%3A%2F%2Flocalhost%3A8080%2F&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&access_type=offline&response_type=code\n",
      "\n",
      "Authentication successful.\n"
     ]
    }
   ],
   "source": [
    "# Instalamos las librerias y las importamos\n",
    "\n",
    "!pip install -U -q PyDrive\n",
    "!pip install -U -q module_name\n",
    "\n",
    "from pydrive.auth import GoogleAuth\n",
    "from pydrive.drive import GoogleDrive\n",
    "import os\n",
    "\n",
    "# Nos autentificamos en Google Drive y creamos la conección.\n",
    "g_login = GoogleAuth()\n",
    "g_login.LocalWebserverAuth()\n",
    "drive = GoogleDrive(g_login)\n",
    "g_login.LocalWebserverAuth()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Abrimos el archivo que queremos subir, y lo subimos a la ubicacion de la carpeta compartida mediante ID\n",
    "with open(file_name,\"r\") as file:\n",
    "    \n",
    "    file_drive = drive.CreateFile({'title':os.path.basename(file.name), \n",
    "                                   \"parents\": [{\"kind\": \"drive#fileLink\", \"id\": \"1-hoNgYsZW1h4gYKY9uAPrJfEAaTOhGCW\"}] })  \n",
    "    file_drive.SetContentString(file.read()) \n",
    "    file_drive.Upload() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
